{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import math\n",
    "'''DATA'''\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "train_ds = TensorDataset(xx, y)\n",
    "val_ds = TensorDataset(xx, y)\n",
    "train_ds[0]\n",
    "from sklearn.model_selection import train_test_split\n",
    "'''DATASETS'''\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    xx, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "bs = 32\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_test, y_test)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.1, 3.5, 1.4, 0.2]), 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"DATASETS\"\"\"\n",
    "x, y = load_iris(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "bs = 32\n",
    "train_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.vocab_size = n\n",
    "        self.embedding_dim = 2\n",
    "        self.l1 = nn.Linear(4,3)\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.flat = nn.Flatten(0,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return self.flat(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/rrossmil/Documents/personal/language-learning/pytorch/embedding.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rrossmil/Documents/personal/language-learning/pytorch/embedding.ipynb#ch0000003?line=41'>42</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m history\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rrossmil/Documents/personal/language-learning/pytorch/embedding.ipynb#ch0000003?line=44'>45</a>\u001b[0m \u001b[39m# model = GCN(g.ndata[\"x\"].shape[1], 64, g.ndata[\"label\"].unique().shape[0])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rrossmil/Documents/personal/language-learning/pytorch/embedding.ipynb#ch0000003?line=45'>46</a>\u001b[0m model \u001b[39m=\u001b[39m NN()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rrossmil/Documents/personal/language-learning/pytorch/embedding.ipynb#ch0000003?line=46'>47</a>\u001b[0m history \u001b[39m=\u001b[39m train(train_dl, valid_dl, model, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'n'"
     ]
    }
   ],
   "source": [
    "def train(train_dl, valid_dl, model, epochs):\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-5\n",
    "    )\n",
    "    history = []\n",
    "    early_stop = 0\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in train_dl:\n",
    "            # Forward\n",
    "            pred = model(x)\n",
    "\n",
    "            # Compute loss\n",
    "            # Note that you should only compute the losses of the nodes in the training set.\n",
    "\n",
    "            loss = F.mse_loss(pred, y)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # end batch\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(F.mse_loss(model(xb), yb) for xb, yb in valid_dl)\n",
    "            valid_loss = valid_loss / len(valid_dl)\n",
    "\n",
    "        history.append(\n",
    "            [\n",
    "                loss.item(),\n",
    "                valid_loss.item(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if e % 100 == 0:\n",
    "            val_acc = 0\n",
    "            print(f\"Epoch {e}, loss: {loss:.3f}, valid_loss {valid_loss:.3f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# model = GCN(g.ndata[\"x\"].shape[1], 64, g.ndata[\"label\"].unique().shape[0])\n",
    "model = NN(3)\n",
    "history = train(train_dl, valid_dl, model, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(history, columns=['loss','valid_loss'])\n",
    "print(df.dtypes)\n",
    "df[['loss','valid_loss']].plot()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "p = model(xx)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xx[:, 0], p.detach().numpy(), label='pred')\n",
    "ax.plot(xx[:, 0], y, label='y')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf23902112f63a418e5641bed0f2752948e7a39f6ebb907d1e9249d165f97b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
