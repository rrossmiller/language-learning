{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#  FROM : https://github.com/dmlc/dgl/tree/master/examples/pytorch/gin\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import dgl\n","from dgl.data import GINDataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# dataset = GINDataset(name=\"IMDBBINARY\", self_loop=False)\n","dataset = GINDataset(\"MUTAG\", self_loop=True)\n","print(len(dataset))\n","print(\"Node feature dimensionality:\", dataset.dim_nfeats)\n","print(\"Number of graph categories:\", dataset.gclasses)\n","dataset[0][0].ndata[\"attr\"][:5]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","\n","\n","def split_fold10(labels, fold_idx=0):\n","    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n","    idx_list = []\n","    for idx in skf.split(np.zeros(len(labels)), labels):\n","        idx_list.append(idx)\n","    train_idx, valid_idx = idx_list[fold_idx]\n","    return train_idx, valid_idx\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from dgl.dataloading import GraphDataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from sklearn.model_selection import train_test_split\n","\n","num_examples = len(dataset)\n","num_train = int(num_examples * 0.8)\n","\n","labels = [l for _, l in dataset]\n","# train_idx, val_idx = split_fold10(labels)\n","train_idx, val_idx = train_test_split(np.arange(num_examples), train_size=num_train, stratify=labels)\n","\n","train_dataloader = GraphDataLoader(\n","    dataset, sampler=SubsetRandomSampler(train_idx), batch_size=5, drop_last=False\n",")\n","test_dataloader = GraphDataLoader(\n","    dataset, sampler=SubsetRandomSampler(val_idx), batch_size=5, drop_last=False\n",")\n","len(train_idx), len(val_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from dgl.nn import GINConv\n","from dgl.nn.pytorch.glob import SumPooling, AvgPooling\n","\n","\n","class MLP(nn.Module):\n","    \"\"\"Construct two-layer MLP-type aggreator for GIN model\"\"\"\n","\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.linears = nn.ModuleList()\n","        # two-layer MLP\n","        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))\n","        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))\n","        self.batch_norm = nn.BatchNorm1d((hidden_dim))\n","\n","    def forward(self, x):\n","        h = x\n","        h = F.relu(self.batch_norm(self.linears[0](h)))\n","        return self.linears[1](h)\n","\n","\n","class GIN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.ginlayers = nn.ModuleList()\n","        self.batch_norms = nn.ModuleList()\n","        num_layers = 5\n","\n","        # five-layer GCN with two-layer MLP aggregator and sum-neighbor-pooling scheme\n","        for layer in range(num_layers - 1):  # excluding the input layer\n","            if layer == 0:\n","                mlp = MLP(input_dim, hidden_dim, hidden_dim)\n","            else:\n","                mlp = MLP(hidden_dim, hidden_dim, hidden_dim)\n","            self.ginlayers.append(\n","                GINConv(mlp, learn_eps=False)\n","            )  # set to True if learning epsilon\n","            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n","        # linear functions for graph sum poolings of output of each layer\n","        self.linear_prediction = nn.ModuleList()\n","        for layer in range(num_layers):\n","            if layer == 0:\n","                self.linear_prediction.append(nn.Linear(input_dim, output_dim))\n","            else:\n","                self.linear_prediction.append(nn.Linear(hidden_dim, output_dim))\n","        self.drop = nn.Dropout(0.5)\n","        self.pool = (\n","            # SumPooling()\n","            AvgPooling()\n","        )  # change to mean readout (AvgPooling) on social network datasets\n","\n","    def forward(self, g, h):\n","        # list of hidden representation at each layer (including the input layer)\n","        hidden_rep = [h]\n","        for i, layer in enumerate(self.ginlayers):\n","            h = layer(g, h)\n","            h = self.batch_norms[i](h)\n","            h = F.relu(h)\n","            hidden_rep.append(h)\n","        score_over_layer = 0\n","        # perform graph sum pooling over all nodes in each layer\n","        for i, h in enumerate(hidden_rep):\n","            pooled_h = self.pool(g, h)\n","            score_over_layer += self.drop(self.linear_prediction[i](pooled_h))\n","        return score_over_layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tqdm import trange\n","from numpy import mean\n","from torch import optim\n","\n","# Create the model with given dimensions\n","model = GIN(dataset.dim_nfeats, 16, dataset.gclasses)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n","\n","tr = trange(200)\n","for epoch in tr:\n","    acc = []\n","\n","    total = 0\n","    total_correct = 0\n","    for batched_graph, labels in train_dataloader:\n","        logits = model(batched_graph, batched_graph.ndata[\"attr\"].float())\n","        loss = F.cross_entropy(logits, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total += len(labels)\n","        total_correct += (logits.argmax(1) == labels).sum().item()\n","    acc = 1.0 * total_correct / total\n","    scheduler.step()\n","\n","    test_acc = []\n","    for batched_graph, labels in test_dataloader:\n","        logits = model(batched_graph, batched_graph.ndata[\"attr\"].float())\n","        total += len(labels)\n","        total_correct += (logits.argmax(1) == labels).sum().item()\n","    test_acc = 1.0 * total_correct / total\n","    tr.set_description(f\"acc: {mean(acc):.4f} | test acc {mean(test_acc):.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","l = test_dataloader.\n","l"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","l, p = [], []\n","for batched_graph, labels in test_dataloader:\n","    pred: torch.Tensor = model(batched_graph, batched_graph.ndata[\"attr\"].float())\n","    l.extend(labels)\n","    p.extend(pred.argmax(1))\n","cm = confusion_matrix(l, p)\n","cm = ConfusionMatrixDisplay(cm)\n","cm.plot(cmap=\"Blues\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
